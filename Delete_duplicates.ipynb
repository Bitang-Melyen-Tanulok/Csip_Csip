{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0b53u3NWTgfsJsEVBDdss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bitang-Melyen-Tanulok/Csip_Csip/blob/main/Delete_duplicates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We needed to check if there is any duplication of titles or audio files among the data.\n",
        "\n",
        "If there was, we would then remove them from the database, only keeping the first appearance."
      ],
      "metadata": {
        "id": "ShXrSdQJGfbt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glXOOUo4GI05"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import shutil\n",
        "\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "def get_file_hash(file_path):\n",
        "    hash_md5 = hashlib.md5()  # Create an MD5 hash object to store the file's hash value\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        # Read the file in chunks (4KB at a time) and update the hash\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()  # Return the final hash value as a hexadecimal string\n",
        "\n",
        "def delete_duplicates_and_log(spectrograms_path):\n",
        "    global X,Y\n",
        "    seen_hashes = set()  # This will store the hashes of files we've already processed\n",
        "    files_to_delete = []  # List to track files that are duplicates and will be deleted\n",
        "    files_to_keep = []  # List to track files that are unique and will be kept\n",
        "\n",
        "    # Walk through the folders in the spectrograms directory\n",
        "    for folder in os.listdir(spectrograms_path):\n",
        "        folder_path = os.path.join(spectrograms_path, folder)\n",
        "\n",
        "        # Loop through the files in the current folder\n",
        "        files = os.listdir(folder_path)\n",
        "        for file in files:\n",
        "            if file.endswith('.png'):  # We're only interested in .png files (spectrograms)\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "\n",
        "                # Generate a hash for the file to compare it with others\n",
        "                file_hash = get_file_hash(file_path)\n",
        "\n",
        "                if file_hash in seen_hashes:\n",
        "                    # If we've already seen this hash, it's a duplicate\n",
        "                    print(f\"Duplicate file found: {file_path}\")\n",
        "                    files_to_delete.append(file_path)\n",
        "                else:\n",
        "                    # If we haven't seen this hash yet, add it to the seen set and keep it\n",
        "                    seen_hashes.add(file_hash)\n",
        "                    files_to_keep.append(file_path)\n",
        "\n",
        "                    img = image.load_img(file_path, target_size=(224, 224), color_mode=\"rgb\")\n",
        "                    img_array = image.img_to_array(img)\n",
        "                    X.append(img_array)\n",
        "                    Y.append(folder)\n",
        "\n",
        "    # Log files that we kept (not duplicates)\n",
        "    print(\"Files kept:\")\n",
        "    for file_path in files_to_keep:\n",
        "        print(f\"   - Keeping file: {file_path}\")\n",
        "\n",
        "    # Log files that we deleted (duplicates)\n",
        "    print(\"\\nFiles deleted:\")\n",
        "    for file_path in files_to_delete:\n",
        "        print(f\"   - Deleting file: {file_path}\")\n",
        "        os.remove(file_path)  # Actually delete the duplicate files\n",
        "\n",
        "    # Convert X and Y to numpy arrays\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    # Shuffle the data\n",
        "    permutation = np.random.permutation(len(X))\n",
        "    X = X[permutation]\n",
        "    Y = Y[permutation]\n",
        "\n",
        "    # Print shapes of the arrays\n",
        "    print(f\"\\nX shape: {X.shape}\")\n",
        "    print(f\"Y shape: {Y.shape}\")\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "X, Y = delete_duplicates_and_log(spectrograms_path)\n",
        "\n"
      ]
    }
  ]
}