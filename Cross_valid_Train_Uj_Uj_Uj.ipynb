{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bitang-Melyen-Tanulok/Csip_Csip/blob/main/Cross_valid_Train_Uj_Uj_Uj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We needed to check if there is any duplication of titles or audio files among the data.\n",
        "\n",
        "If there was, we would then remove them from the database, only keeping the first appearance.\n"
      ],
      "metadata": {
        "id": "g7lCiRbeAAYs"
      }
    },
    {
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/DeepLearning'\n",
        "spectrograms_path = os.path.join(base_path, 'ugyanaz?')  # Directory for spectrogram files\n",
        "\n",
        "import hashlib\n",
        "import shutil\n",
        "\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "def get_file_hash(file_path):\n",
        "    hash_md5 = hashlib.md5()  # Create an MD5 hash object to store the file's hash value\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        # Read the file in chunks (4KB at a time) and update the hash\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()  # Return the final hash value as a hexadecimal string\n",
        "\n",
        "def delete_duplicates_and_log(spectrograms_path):\n",
        "    global X,Y\n",
        "    seen_hashes = set()  # This will store the hashes of files we've already processed\n",
        "    files_to_delete = []  # List to track files that are duplicates and will be deleted\n",
        "    files_to_keep = []  # List to track files that are unique and will be kept\n",
        "\n",
        "    # Walk through the folders in the spectrograms directory\n",
        "    for folder in os.listdir(spectrograms_path):\n",
        "        folder_path = os.path.join(spectrograms_path, folder)\n",
        "\n",
        "        # Loop through the files in the current folder\n",
        "        files = os.listdir(folder_path)\n",
        "        for file in files:\n",
        "            if file.endswith('.png'):  # We're only interested in .png files (spectrograms)\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "\n",
        "                # Generate a hash for the file to compare it with others\n",
        "                file_hash = get_file_hash(file_path)\n",
        "\n",
        "                if file_hash in seen_hashes:\n",
        "                    # If we've already seen this hash, it's a duplicate\n",
        "                    #print(f\"Duplicate file found: {file_path}\")\n",
        "                    files_to_delete.append(file_path)\n",
        "                else:\n",
        "                    # If we haven't seen this hash yet, add it to the seen set and keep it\n",
        "                    seen_hashes.add(file_hash)\n",
        "                    files_to_keep.append(file_path)\n",
        "\n",
        "                    img = image.load_img(file_path, color_mode=\"rgb\")\n",
        "                    img_array = image.img_to_array(img)\n",
        "                    X.append(img_array)\n",
        "                    Y.append(folder)\n",
        "\n",
        "    # Log files that we kept (not duplicates)\n",
        "    '''\n",
        "    print(\"Files kept:\")\n",
        "    for file_path in files_to_keep:\n",
        "        print(f\"   - Keeping file: {file_path}\")\n",
        "    '''\n",
        "\n",
        "    # Log files that we deleted (duplicates)\n",
        "    print(\"\\nFiles deleted:\")\n",
        "    for file_path in files_to_delete:\n",
        "        print(f\"   - Deleting file: {file_path}\")\n",
        "        os.remove(file_path)  # Actually delete the duplicate files\n",
        "\n",
        "    # Convert X and Y to numpy arrays\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    # Shuffle the data\n",
        "    permutation = np.random.permutation(len(X))\n",
        "    X = X[permutation]\n",
        "    Y = Y[permutation]\n",
        "\n",
        "    # Print shapes of the arrays\n",
        "    print(f\"\\nX shape: {X.shape}\")\n",
        "    print(f\"Y shape: {Y.shape}\")\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "X, Y = delete_duplicates_and_log(spectrograms_path)\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9wkFCOwSKw7",
        "outputId": "536d9174-b24b-4526-ce86-aede233ac18f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Files deleted:\n",
            "\n",
            "X shape: (1763, 128, 313, 3)\n",
            "Y shape: (1763,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmentáció segédfüggvényei"
      ],
      "metadata": {
        "id": "Gux4KbJg1nIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify underrepresented classes\n",
        "from collections import Counter\n",
        "\n",
        "class_counts = Counter(Y)\n",
        "underrepresented_classes = [cls for cls, count in class_counts.items() if count < 29]\n",
        "print(f\"Underrepresented classes: {underrepresented_classes}\")"
      ],
      "metadata": {
        "id": "LE6gpXCE2Bah",
        "outputId": "d4e2a707-55c7-4fb9-bd1d-8c971448e8db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Underrepresented classes: ['forwag1', 'vefnut1', 'jerbus2', 'redspu1', 'plaflo1', 'lewduc1', 'sqtbul1', 'rutfly6', 'rufbab3', 'junmyn1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations"
      ],
      "metadata": {
        "id": "7t0jezy85BQU",
        "outputId": "e6badf5c-94b4-4209-d0be-7dcc8ddcabc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations\n",
            "  Downloading albumentations-1.4.24-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.10.3)\n",
            "Collecting albucore==0.0.23 (from albumentations)\n",
            "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting stringzilla>=3.10.4 (from albucore==0.0.23->albumentations)\n",
            "  Downloading stringzilla-3.11.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.1/80.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n",
            "  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n",
            "Downloading albumentations-1.4.24-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\n",
            "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stringzilla-3.11.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stringzilla, simsimd, opencv-python-headless, albucore, albumentations\n",
            "Successfully installed albucore-0.0.23 albumentations-1.4.24 opencv-python-headless-4.10.0.84 simsimd-6.2.1 stringzilla-3.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "\n",
        "coarse_dropout = A.CoarseDropout(max_holes=1, max_height=32, max_width=32, p=0.5)\n",
        "\n",
        "def apply_coarse_dropout(image):\n",
        "    return coarse_dropout(image=image)[\"image\"]"
      ],
      "metadata": {
        "id": "Iw4GU85P1raL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def time_mask(image, mask_width=10):\n",
        "    image = image.copy()\n",
        "    t_start = random.randint(0, image.shape[1] - mask_width)\n",
        "    image[:, t_start:t_start + mask_width] = 0\n",
        "    return image"
      ],
      "metadata": {
        "id": "xfZI9EzK1tqN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frequency_mask(image, mask_height=10):\n",
        "    image = image.copy()\n",
        "    f_start = random.randint(0, image.shape[0] - mask_height)\n",
        "    image[f_start:f_start + mask_height, :] = 0\n",
        "    return image"
      ],
      "metadata": {
        "id": "nIlnud6G1w2U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is a multi-class classification task, I am converting labels to one-hot format:"
      ],
      "metadata": {
        "id": "CMxgjW0YKfqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#First, the labels need to be converted into numerical values\n",
        "le = LabelEncoder()\n",
        "Y_encoded = le.fit_transform(Y)\n",
        "\n",
        "#Getting number of classes\n",
        "num_classes = len(le.classes_)\n",
        "print(f\"Class number= {num_classes}\")\n",
        "\n",
        "#Converting to one-hot encoding\n",
        "Y_onehot = to_categorical(Y_encoded, num_classes)"
      ],
      "metadata": {
        "id": "8Wr1YIvJXB2e",
        "outputId": "f745954d-21d8-4e18-868f-7979a695aa76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class number= 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spektrogramok augmentálása"
      ],
      "metadata": {
        "id": "CZ-yDP8Z5csT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_X = []\n",
        "augmented_Y = []\n",
        "\n",
        "for cls in underrepresented_classes:\n",
        "    class_indices = np.where(Y == cls)[0]\n",
        "    class_samples = X[class_indices]\n",
        "    class_labels = Y_onehot[class_indices]\n",
        "\n",
        "    while len(class_samples) < 29:\n",
        "        for sample, label in zip(class_samples, class_labels):\n",
        "            # Apply augmentations\n",
        "            aug_sample = sample.copy()\n",
        "\n",
        "            # Randomly apply augmentations\n",
        "            if np.random.rand() > 0.5:\n",
        "                aug_sample = apply_coarse_dropout(aug_sample)\n",
        "            if np.random.rand() > 0.5:\n",
        "                aug_sample = time_mask(aug_sample)\n",
        "            if np.random.rand() > 0.5:\n",
        "                aug_sample = frequency_mask(aug_sample)\n",
        "\n",
        "            augmented_X.append(aug_sample)\n",
        "            augmented_Y.append(label)\n",
        "\n",
        "            if len(class_samples) + len(augmented_X) >= 29:\n",
        "                break\n",
        "\n",
        "# Convert augmented data to arrays\n",
        "augmented_X = np.array(augmented_X)\n",
        "augmented_Y = np.array(augmented_Y)"
      ],
      "metadata": {
        "id": "SwongAYN5TME",
        "outputId": "428f12a3-7175-4b78-9cdc-62f7080a6a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'underrepresented_classes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e511de3978a3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maugmented_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munderrepresented_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mclass_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mclass_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'underrepresented_classes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add augmented samples to the main dataset\n",
        "X = np.concatenate((X, augmented_X), axis=0)\n",
        "Y_onehot = np.concatenate((Y_onehot, augmented_Y), axis=0)\n",
        "\n",
        "#Shuffle\n",
        "permutation = np.random.permutation(len(X))\n",
        "X = X[permutation]\n",
        "Y_onehot = Y_onehot[permutation]"
      ],
      "metadata": {
        "id": "WayenJTr5U9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented early stopping, in case the model started overfitting."
      ],
      "metadata": {
        "id": "qbrOkU8JAf8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "#Implementing early stopping, since there is no reason for it to learn further when val_loss isn't decreasing\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10, #If it doesn't improve for 10 epochs, it concludes\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "sDYfYQU49emi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint # Importing the missing ModelCheckpoint class\n",
        "\n",
        "#Wanting to save the best model, so implementing checkpointing\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "_F0hjbhk9guq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet"
      ],
      "metadata": {
        "id": "Hk4oZTN7zXcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used efficientNetV2 pretrained model, since most kaggle teams suggested it."
      ],
      "metadata": {
        "id": "1wkKtEyyAycC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "\n",
        "# Setting up Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_accuracies = []\n",
        "fold_losses = []\n",
        "history_per_fold = []\n",
        "\n",
        "# Starting K-Fold\n",
        "fold_number = 1\n",
        "for train_index, valid_index in skf.split(X, Y_encoded):\n",
        "    print(f\"Starting fold: {fold_number}\")\n",
        "\n",
        "    # Splitting the data into training and validation sets\n",
        "    X_train, X_valid = X[train_index], X[valid_index]\n",
        "    Y_train, Y_valid = Y_onehot[train_index], Y_onehot[valid_index]\n",
        "\n",
        "    # Normalizing the data\n",
        "    X_train = preprocess_input(X_train) #X_train = X_train / np.max(X_train)\n",
        "    X_valid = preprocess_input(X_valid) #X_valid = X_valid / np.max(X_valid)\n",
        "\n",
        "    # Setting the input shape based on the data\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
        "\n",
        "    # Loading the pre-trained EfficientNetV2B0 model\n",
        "    base_model = EfficientNetV2B0(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    # Defining a new model\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Freezing the base model layers if we don't want to update them during training\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Adding the pre-trained model\n",
        "    model.add(base_model)\n",
        "    model.add(layers.GlobalAveragePooling2D())  # Selecting the best representations\n",
        "\n",
        "    # Adding Dense layers\n",
        "    # Underfitting esetén Dense 256 -> Dropout 0.4 -> Dende 128 -> Dropout 0.4 ?\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())  # Adding Batch Normalization\n",
        "    model.add(layers.Dropout(0.5))  # Adding Dropout to prevent overfitting\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))  # Output layer\n",
        "\n",
        "\n",
        "    # Compiling the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training the model\n",
        "    history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        epochs=10,\n",
        "        batch_size=64,\n",
        "        validation_data=(X_valid, Y_valid),\n",
        "        callbacks=[early_stopping, checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Fine-tuning\n",
        "    base_model.trainable = True\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    fine_tuning_history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        validation_data=(X_valid, Y_valid),\n",
        "        callbacks=[early_stopping, checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Saving the results\n",
        "    history_per_fold.append((history, fine_tuning_history))\n",
        "    valid_loss, valid_accuracy = model.evaluate(X_valid, Y_valid)\n",
        "\n",
        "    fold_accuracies.append(valid_accuracy)\n",
        "    fold_losses.append(valid_loss)\n",
        "\n",
        "    fold_number += 1\n",
        "\n",
        "# Summary of the final model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GkRTUHa6XL8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCross-Validation Results:\")\n",
        "print(f\"Mean Validation Loss: {np.mean(fold_losses)}\")\n",
        "print(f\"Mean Validation Accuracy: {np.mean(fold_accuracies)}\")\n",
        "\n",
        "for i, (loss, accuracy) in enumerate(zip(fold_losses, fold_accuracies), start=1):\n",
        "    print(f\"Fold {i} - Validation Loss: {loss}, Validation Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "IG5spDQxY_e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss and accuracy for each fold\n",
        "for i, history in enumerate(history_per_fold):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  plt.title(f'Fold {i + 1} Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot loss\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title(f'Fold {i + 1} Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "pYMtUWV_ETgQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}