{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bitang-Melyen-Tanulok/Csip_Csip/blob/main/Cross_valid_Train_Uj_Uj_Uj_mentessel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We needed to check if there is any duplication of titles or audio files among the data.\n",
        "\n",
        "If there was, we would then remove them from the database, only keeping the first appearance.\n"
      ],
      "metadata": {
        "id": "g7lCiRbeAAYs"
      }
    },
    {
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/DeepLearning'\n",
        "spectrograms_path = os.path.join(base_path, 'ugyanaz?')  # Directory for spectrogram files\n",
        "\n",
        "import hashlib\n",
        "import shutil\n",
        "\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "def get_file_hash(file_path):\n",
        "    hash_md5 = hashlib.md5()  # Create an MD5 hash object to store the file's hash value\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        # Read the file in chunks (4KB at a time) and update the hash\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()  # Return the final hash value as a hexadecimal string\n",
        "\n",
        "def delete_duplicates_and_log(spectrograms_path):\n",
        "    global X,Y\n",
        "    seen_hashes = set()  # This will store the hashes of files we've already processed\n",
        "    files_to_delete = []  # List to track files that are duplicates and will be deleted\n",
        "    files_to_keep = []  # List to track files that are unique and will be kept\n",
        "\n",
        "    # Walk through the folders in the spectrograms directory\n",
        "    for folder in os.listdir(spectrograms_path):\n",
        "        folder_path = os.path.join(spectrograms_path, folder)\n",
        "\n",
        "        # Loop through the files in the current folder\n",
        "        files = os.listdir(folder_path)\n",
        "        for file in files:\n",
        "            if file.endswith('.png'):  # We're only interested in .png files (spectrograms)\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "\n",
        "                # Generate a hash for the file to compare it with others\n",
        "                file_hash = get_file_hash(file_path)\n",
        "\n",
        "                if file_hash in seen_hashes:\n",
        "                    # If we've already seen this hash, it's a duplicate\n",
        "                    #print(f\"Duplicate file found: {file_path}\")\n",
        "                    files_to_delete.append(file_path)\n",
        "                else:\n",
        "                    # If we haven't seen this hash yet, add it to the seen set and keep it\n",
        "                    seen_hashes.add(file_hash)\n",
        "                    files_to_keep.append(file_path)\n",
        "\n",
        "                    img = image.load_img(file_path, color_mode=\"rgb\")\n",
        "                    img_array = image.img_to_array(img)\n",
        "                    X.append(img_array)\n",
        "                    Y.append(folder)\n",
        "\n",
        "    # Log files that we kept (not duplicates)\n",
        "    '''\n",
        "    print(\"Files kept:\")\n",
        "    for file_path in files_to_keep:\n",
        "        print(f\"   - Keeping file: {file_path}\")\n",
        "    '''\n",
        "\n",
        "    # Log files that we deleted (duplicates)\n",
        "    print(\"\\nFiles deleted:\")\n",
        "    for file_path in files_to_delete:\n",
        "        print(f\"   - Deleting file: {file_path}\")\n",
        "        os.remove(file_path)  # Actually delete the duplicate files\n",
        "\n",
        "    # Convert X and Y to numpy arrays\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    # Shuffle the data\n",
        "    permutation = np.random.permutation(len(X))\n",
        "    X = X[permutation]\n",
        "    Y = Y[permutation]\n",
        "\n",
        "    # Print shapes of the arrays\n",
        "    print(f\"\\nX shape: {X.shape}\")\n",
        "    print(f\"Y shape: {Y.shape}\")\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "X, Y = delete_duplicates_and_log(spectrograms_path)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9wkFCOwSKw7",
        "outputId": "5ddb727b-2da1-4fec-c71a-8939e0a6f784"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Files deleted:\n",
            "\n",
            "X shape: (1769, 128, 313, 3)\n",
            "Y shape: (1769,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is a multi-class classification task, I am converting labels to one-hot format:"
      ],
      "metadata": {
        "id": "CMxgjW0YKfqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#First, the labels need to be converted into numerical values\n",
        "le = LabelEncoder()\n",
        "Y_encoded = le.fit_transform(Y)\n",
        "\n",
        "#Getting number of classes\n",
        "num_classes = len(le.classes_)\n",
        "print(f\"Class number= {num_classes}\")\n",
        "\n",
        "#Converting to one-hot encoding\n",
        "Y_onehot = to_categorical(Y_encoded, num_classes)"
      ],
      "metadata": {
        "id": "8Wr1YIvJXB2e",
        "outputId": "e853245d-57b9-441e-b3c0-e1700f72d28d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class number= 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented early stopping, in case the model started overfitting."
      ],
      "metadata": {
        "id": "qbrOkU8JAf8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "#Implementing early stopping, since there is no reason for it to learn further when val_loss isn't decreasing\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5, #If it doesn't improve for 10 epochs, it concludes\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "sDYfYQU49emi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint # Importing the missing ModelCheckpoint class\n",
        "\n",
        "checkpoint_dir = '/content/drive/MyDrive/DeepLearning/checkpoints_sample'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "#checkpoint_path = os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}_val_loss_{val_loss:.4f}.keras')\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'model_latest.keras')\n",
        "epoch_info_path = os.path.join(checkpoint_dir, 'epoch_info.npy')\n",
        "\n",
        "#Saving the latest model at each epoch, so that if colab collapses, we don't have to start from zero\n",
        "checkpoint = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "_F0hjbhk9guq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk4oZTN7zXcO",
        "outputId": "eef2a3e9-7879-4929-9acb-1f50c7042123"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting efficientnet\n",
            "  Downloading efficientnet-1.1.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from efficientnet) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.12.1)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (11.0.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (2024.12.12)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (0.4)\n",
            "Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-applications, efficientnet\n",
            "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used efficientNetV2 pretrained model, since most kaggle teams suggested it."
      ],
      "metadata": {
        "id": "1wkKtEyyAycC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable XLA compilation for faster execution\n",
        "# tf.config.optimizer.set_jit(True)\n",
        "\n",
        "# Setting up Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Implementing mixed precision training for faster outcomes\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Setting up learning rate sceduler - reduces learning rat if the model stops improving\n",
        "# lr_scheduler = ReduceLROnPlateau(\n",
        "#     monitor='val_loss',\n",
        "#     factor=0.5,\n",
        "#     patience=3,\n",
        "#     verbose=1,\n",
        "#     min_lr=1e-6\n",
        "# )\n",
        "\n",
        "fold_accuracies = []\n",
        "fold_losses = []\n",
        "history_per_fold = []\n",
        "\n",
        "last_completed_epoch = 0\n",
        "if os.path.exists(epoch_info_path) and os.path.exists(checkpoint_path):\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    last_completed_epoch = int(np.load(epoch_info_path))\n",
        "    print(f\"Resuming from epoch {last_completed_epoch}\")\n",
        "else:\n",
        "  print(\"No epoch info file found. Starting from scratch.\")\n",
        "\n",
        "# Starting K-Fold\n",
        "fold_number = 1\n",
        "for train_index, valid_index in skf.split(X, Y_encoded):\n",
        "    print(f\"Starting fold: {fold_number}\")\n",
        "\n",
        "    # Splitting the data into training and validation sets\n",
        "    X_train, X_valid = X[train_index], X[valid_index]\n",
        "    Y_train, Y_valid = Y_onehot[train_index], Y_onehot[valid_index]\n",
        "\n",
        "    # Normalizing the data\n",
        "    X_train = preprocess_input(X_train)\n",
        "    X_valid = preprocess_input(X_valid)\n",
        "\n",
        "    # Setting the input shape based on the data\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
        "\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    if latest_checkpoint:\n",
        "      print(f\"Loading latest checkpoint: {latest_checkpoint}\")\n",
        "      model = load_model(latest_checkpoint)\n",
        "    else:\n",
        "      print(\"No checkpoint found. Building new model.\")\n",
        "\n",
        "      # Loading the pre-trained EfficientNetV2B0 model\n",
        "      base_model = EfficientNetV2B0(\n",
        "          include_top=False,\n",
        "          weights='imagenet',\n",
        "          input_shape=input_shape\n",
        "      )\n",
        "\n",
        "      # Defining a new model\n",
        "      model = models.Sequential()\n",
        "\n",
        "      # Freezing the base model layers if we don't want to update them during training\n",
        "      base_model.trainable = False\n",
        "\n",
        "      # Adding the pre-trained model\n",
        "      model.add(base_model)\n",
        "      model.add(layers.GlobalAveragePooling2D())  # Selecting the best representations\n",
        "\n",
        "      # Adding Dense layers\n",
        "      model.add(layers.Dense(128, activation='relu')) #128\n",
        "      model.add(layers.BatchNormalization())\n",
        "      model.add(layers.Dropout(0.4))  # Adding Dropout to prevent overfitting\n",
        "      model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "      # Compiling the model\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training the model\n",
        "    history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        initial_epoch=last_completed_epoch,\n",
        "        epochs=20,\n",
        "        batch_size=128, # try larger throughput 128, 256, original is 64\n",
        "        validation_data=(X_valid, Y_valid),\n",
        "        callbacks=[early_stopping, checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Save the last completed epoch\n",
        "    last_completed_epoch += len(history.epoch)\n",
        "    np.save(epoch_info_path, last_completed_epoch)\n",
        "\n",
        "    # Fine-tuning the last 10 layers\n",
        "    base_model.trainable = False\n",
        "    for layer in base_model.layers[-10:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy']) # low learning rate\n",
        "\n",
        "    fine_tuning_history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        epochs=5,\n",
        "        batch_size=128,\n",
        "        validation_data=(X_valid, Y_valid),\n",
        "        callbacks=[early_stopping, checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Saving the results\n",
        "    final_model_path = os.path.join(checkpoint_dir, f'final_model_fold_{fold_number}.keras')\n",
        "    model.save(final_model_path)\n",
        "\n",
        "    valid_loss, valid_accuracy = model.evaluate(X_valid, Y_valid)\n",
        "\n",
        "    fold_accuracies.append(valid_accuracy)\n",
        "    fold_losses.append(valid_loss)\n",
        "    history_per_fold.append(history)\n",
        "\n",
        "    fold_number += 1\n",
        "\n",
        "# Summary of the final model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GkRTUHa6XL8L",
        "outputId": "91d7525f-7a5d-43e4-ec2f-110712b3da35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from epoch 20\n",
            "Starting fold: 1\n",
            "No checkpoint found. Building new model.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "24274472/24274472 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.1460 - accuracy: 0.0565 \n",
            "Epoch 1: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 156s 12s/step - loss: 4.1460 - accuracy: 0.0565 - val_loss: 2.9176 - val_accuracy: 0.1215\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.0412 - accuracy: 0.0565 \n",
            "Epoch 2: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 142s 12s/step - loss: 4.0412 - accuracy: 0.0565 - val_loss: 2.9197 - val_accuracy: 0.1158\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.0838 - accuracy: 0.0594 \n",
            "Epoch 3: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 141s 12s/step - loss: 4.0838 - accuracy: 0.0594 - val_loss: 2.9294 - val_accuracy: 0.0989\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.0333 - accuracy: 0.0636 \n",
            "Epoch 4: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 140s 12s/step - loss: 4.0333 - accuracy: 0.0636 - val_loss: 2.9261 - val_accuracy: 0.1045\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.9667 - accuracy: 0.0551 \n",
            "Epoch 5: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 141s 12s/step - loss: 3.9667 - accuracy: 0.0551 - val_loss: 2.9133 - val_accuracy: 0.1102\n",
            "12/12 [==============================] - 32s 3s/step - loss: 2.9129 - accuracy: 0.1073\n",
            "Starting fold: 2\n",
            "No checkpoint found. Building new model.\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.0089 - accuracy: 0.0636 \n",
            "Epoch 1: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 154s 12s/step - loss: 4.0089 - accuracy: 0.0636 - val_loss: 2.9775 - val_accuracy: 0.0734\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.9562 - accuracy: 0.0678 \n",
            "Epoch 2: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 142s 12s/step - loss: 3.9562 - accuracy: 0.0678 - val_loss: 2.9790 - val_accuracy: 0.1017\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.8710 - accuracy: 0.0629 \n",
            "Epoch 3: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 142s 12s/step - loss: 3.8710 - accuracy: 0.0629 - val_loss: 2.9779 - val_accuracy: 0.1073\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.8766 - accuracy: 0.0742 \n",
            "Epoch 4: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 142s 12s/step - loss: 3.8766 - accuracy: 0.0742 - val_loss: 2.9704 - val_accuracy: 0.1102\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.8132 - accuracy: 0.0763 \n",
            "Epoch 5: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 141s 12s/step - loss: 3.8132 - accuracy: 0.0763 - val_loss: 2.9626 - val_accuracy: 0.1271\n",
            "12/12 [==============================] - 32s 3s/step - loss: 2.9619 - accuracy: 0.1271\n",
            "Starting fold: 3\n",
            "No checkpoint found. Building new model.\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.2585 - accuracy: 0.0523 \n",
            "Epoch 1: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 152s 12s/step - loss: 4.2585 - accuracy: 0.0523 - val_loss: 3.2046 - val_accuracy: 0.0198\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.3239 - accuracy: 0.0537 \n",
            "Epoch 2: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 140s 12s/step - loss: 4.3239 - accuracy: 0.0537 - val_loss: 3.2083 - val_accuracy: 0.0169\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.2022 - accuracy: 0.0629\n",
            "Epoch 3: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 138s 12s/step - loss: 4.2022 - accuracy: 0.0629 - val_loss: 3.2005 - val_accuracy: 0.0113\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.2036 - accuracy: 0.0572 \n",
            "Epoch 4: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 140s 12s/step - loss: 4.2036 - accuracy: 0.0572 - val_loss: 3.1836 - val_accuracy: 0.0141\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.1509 - accuracy: 0.0671 \n",
            "Epoch 5: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 140s 12s/step - loss: 4.1509 - accuracy: 0.0671 - val_loss: 3.1672 - val_accuracy: 0.0198\n",
            "12/12 [==============================] - 31s 3s/step - loss: 3.1666 - accuracy: 0.0198\n",
            "Starting fold: 4\n",
            "No checkpoint found. Building new model.\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.2409 - accuracy: 0.0396 \n",
            "Epoch 1: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 154s 12s/step - loss: 4.2409 - accuracy: 0.0396 - val_loss: 3.0297 - val_accuracy: 0.0508\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.1519 - accuracy: 0.0495 \n",
            "Epoch 2: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 141s 12s/step - loss: 4.1519 - accuracy: 0.0495 - val_loss: 3.0132 - val_accuracy: 0.0650\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.1158 - accuracy: 0.0495 \n",
            "Epoch 3: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 140s 12s/step - loss: 4.1158 - accuracy: 0.0495 - val_loss: 3.0066 - val_accuracy: 0.0650\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.0659 - accuracy: 0.0516\n",
            "Epoch 4: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 139s 12s/step - loss: 4.0659 - accuracy: 0.0516 - val_loss: 3.0034 - val_accuracy: 0.0593\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.9594 - accuracy: 0.0537\n",
            "Epoch 5: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 139s 12s/step - loss: 3.9594 - accuracy: 0.0537 - val_loss: 2.9851 - val_accuracy: 0.0706\n",
            "12/12 [==============================] - 31s 3s/step - loss: 2.9850 - accuracy: 0.0706\n",
            "Starting fold: 5\n",
            "No checkpoint found. Building new model.\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.2016 - accuracy: 0.0459 \n",
            "Epoch 1: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 153s 12s/step - loss: 4.2016 - accuracy: 0.0459 - val_loss: 3.0332 - val_accuracy: 0.0453\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.0824 - accuracy: 0.0650 \n",
            "Epoch 2: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 141s 12s/step - loss: 4.0824 - accuracy: 0.0650 - val_loss: 3.0204 - val_accuracy: 0.0425\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 4.0265 - accuracy: 0.0678 \n",
            "Epoch 3: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 140s 12s/step - loss: 4.0265 - accuracy: 0.0678 - val_loss: 3.0158 - val_accuracy: 0.0510\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.9345 - accuracy: 0.0600 \n",
            "Epoch 4: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 141s 12s/step - loss: 3.9345 - accuracy: 0.0600 - val_loss: 3.0066 - val_accuracy: 0.0567\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.8659 - accuracy: 0.0734 \n",
            "Epoch 5: saving model to /content/drive/MyDrive/DeepLearning/checkpoints_sample/model_latest.keras\n",
            "12/12 [==============================] - 140s 12s/step - loss: 3.8659 - accuracy: 0.0734 - val_loss: 2.9945 - val_accuracy: 0.0595\n",
            "12/12 [==============================] - 31s 3s/step - loss: 2.9932 - accuracy: 0.0595\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnetv2-b0 (Functio  (None, 4, 10, 1280)       5919312   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " global_average_pooling2d_4  (None, 1280)              0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               163968    \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 20)                2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6086372 (23.22 MB)\n",
            "Trainable params: 166804 (651.58 KB)\n",
            "Non-trainable params: 5919568 (22.58 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCross-Validation Results:\")\n",
        "print(f\"Mean Validation Loss: {np.mean(fold_losses)}\")\n",
        "print(f\"Mean Validation Accuracy: {np.mean(fold_accuracies)}\")\n",
        "\n",
        "for i, (loss, accuracy) in enumerate(zip(fold_losses, fold_accuracies), start=1):\n",
        "    print(f\"Fold {i} - Validation Loss: {loss}, Validation Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "IG5spDQxY_e3",
        "outputId": "4ca1034c-8fe0-4a48-88fd-89510f5ba8ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Validation Results:\n",
            "Mean Validation Loss: 3.003914546966553\n",
            "Mean Validation Accuracy: 0.0768697690218687\n",
            "Fold 1 - Validation Loss: 2.9128596782684326, Validation Accuracy: 0.10734463483095169\n",
            "Fold 2 - Validation Loss: 2.9619085788726807, Validation Accuracy: 0.12711864709854126\n",
            "Fold 3 - Validation Loss: 3.1665894985198975, Validation Accuracy: 0.01977401040494442\n",
            "Fold 4 - Validation Loss: 2.9850149154663086, Validation Accuracy: 0.07062146812677383\n",
            "Fold 5 - Validation Loss: 2.9932000637054443, Validation Accuracy: 0.059490084648132324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot loss and accuracy for each fold\n",
        "for i, history in enumerate(history_per_fold):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  plt.title(f'Fold {i + 1} Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot loss\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title(f'Fold {i + 1} Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "pYMtUWV_ETgQ",
        "outputId": "0cdfd4fa-3a7f-4f6a-d22e-248037e5ea17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'accuracy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5b3180106e04>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Plot accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Fold {i + 1} Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAH/CAYAAABpfcWfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdRklEQVR4nO3df2zX9Z3A8Vcp9lvNbGXHUX5cHac75zYnOJCuOmJceiPRsOOPyzhdgCNOz40zjuZugj/onBvlnBqSiSMyPZfcPNgZ9ZZB6rneyOLkQgY0cSdqHDq4Za1wO1qGWyvt5/5Y7NYBjm9t4UV5PJLvH337/nw/7+873Z79fH/wrSiKoggA4JQbd6oXAAD8ligDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASZUf5hz/8YcyfPz+mTp0aFRUV8fTTT//RY7Zu3Rof/ehHo1Qqxfvf//547LHHhrFUABjbyo7y4cOHY8aMGbFu3boTmv/aa6/FtddeG1dffXV0dHTEF77whfjsZz8bzzzzTNmLBYCxrOLdfCFFRUVFPPXUU7FgwYLjzrntttti8+bN8ZOf/GRw7G/+5m/i4MGD0dbWNtxTA8CYM360T7Bt27ZoamoaMjZv3rz4whe+cNxjent7o7e3d/DngYGB+OUvfxl/8id/EhUVFaO1VAA4IUVRxKFDh2Lq1KkxbtzIvT1r1KPc2dkZdXV1Q8bq6uqip6cnfv3rX8fZZ5991DGtra1x9913j/bSAOBd2bdvX/zZn/3ZiN3fqEd5OFauXBnNzc2DP3d3d8f5558f+/bti5qamlO4MgCI6Onpifr6+jj33HNH9H5HPcqTJ0+Orq6uIWNdXV1RU1NzzKvkiIhSqRSlUumo8ZqaGlEGII2Rfkl11D+n3NjYGO3t7UPGnn322WhsbBztUwPAaaXsKP/qV7+Kjo6O6OjoiIjffuSpo6Mj9u7dGxG/fep58eLFg/Nvvvnm2LNnT3zxi1+Ml156KR566KH4zne+E8uXLx+ZRwAAY0TZUf7xj38cl112WVx22WUREdHc3ByXXXZZrFq1KiIifvGLXwwGOiLiz//8z2Pz5s3x7LPPxowZM+L++++Pb37zmzFv3rwReggAMDa8q88pnyw9PT1RW1sb3d3dXlMG4JQbrS75t68BIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASCJYUV53bp1MX369Kiuro6GhobYvn37O85fu3ZtfOADH4izzz476uvrY/ny5fGb3/xmWAsGgLGq7Chv2rQpmpubo6WlJXbu3BkzZsyIefPmxRtvvHHM+Y8//nisWLEiWlpaYvfu3fHII4/Epk2b4vbbb3/XiweAsaTsKD/wwANx4403xtKlS+NDH/pQrF+/Ps4555x49NFHjzn/+eefjyuvvDKuv/76mD59enzyk5+M66677o9eXQPAmaasKPf19cWOHTuiqanpd3cwblw0NTXFtm3bjnnMFVdcETt27BiM8J49e2LLli1xzTXXvItlA8DYM76cyQcOHIj+/v6oq6sbMl5XVxcvvfTSMY+5/vrr48CBA/Hxj388iqKII0eOxM033/yOT1/39vZGb2/v4M89PT3lLBMATkuj/u7rrVu3xurVq+Ohhx6KnTt3xpNPPhmbN2+Oe+6557jHtLa2Rm1t7eCtvr5+tJcJAKdcRVEUxYlO7uvri3POOSeeeOKJWLBgweD4kiVL4uDBg/Hv//7vRx0zd+7c+NjHPhZf+9rXBsf+5V/+JW666ab41a9+FePGHf13wbGulOvr66O7uztqampOdLkAMCp6enqitrZ2xLtU1pVyVVVVzJo1K9rb2wfHBgYGor29PRobG495zJtvvnlUeCsrKyMi4nh/D5RKpaipqRlyA4CxrqzXlCMimpubY8mSJTF79uyYM2dOrF27Ng4fPhxLly6NiIjFixfHtGnTorW1NSIi5s+fHw888EBcdtll0dDQEK+++mrcddddMX/+/ME4AwDDiPLChQtj//79sWrVqujs7IyZM2dGW1vb4Ju/9u7dO+TK+M4774yKioq488474+c//3n86Z/+acyfPz+++tWvjtyjAIAxoKzXlE+V0XruHgCGI8VrygDA6BFlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIYlhRXrduXUyfPj2qq6ujoaEhtm/f/o7zDx48GMuWLYspU6ZEqVSKiy66KLZs2TKsBQPAWDW+3AM2bdoUzc3NsX79+mhoaIi1a9fGvHnz4uWXX45JkyYdNb+vry/+8i//MiZNmhRPPPFETJs2LX72s5/FeeedNxLrB4Axo6IoiqKcAxoaGuLyyy+PBx98MCIiBgYGor6+Pm655ZZYsWLFUfPXr18fX/va1+Kll16Ks846a1iL7Onpidra2uju7o6ampph3QcAjJTR6lJZT1/39fXFjh07oqmp6Xd3MG5cNDU1xbZt2455zHe/+91obGyMZcuWRV1dXVxyySWxevXq6O/vP+55ent7o6enZ8gNAMa6sqJ84MCB6O/vj7q6uiHjdXV10dnZecxj9uzZE0888UT09/fHli1b4q677or7778/vvKVrxz3PK2trVFbWzt4q6+vL2eZAHBaGvV3Xw8MDMSkSZPi4YcfjlmzZsXChQvjjjvuiPXr1x/3mJUrV0Z3d/fgbd++faO9TAA45cp6o9fEiROjsrIyurq6hox3dXXF5MmTj3nMlClT4qyzzorKysrBsQ9+8IPR2dkZfX19UVVVddQxpVIpSqVSOUsDgNNeWVfKVVVVMWvWrGhvbx8cGxgYiPb29mhsbDzmMVdeeWW8+uqrMTAwMDj2yiuvxJQpU44ZZAA4U5X99HVzc3Ns2LAhvvWtb8Xu3bvjc5/7XBw+fDiWLl0aERGLFy+OlStXDs7/3Oc+F7/85S/j1ltvjVdeeSU2b94cq1evjmXLlo3cowCAMaDszykvXLgw9u/fH6tWrYrOzs6YOXNmtLW1Db75a+/evTFu3O9aX19fH88880wsX748Lr300pg2bVrceuutcdttt43cowCAMaDszymfCj6nDEAmKT6nDACMHlEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIAlRBoAkRBkAkhBlAEhClAEgCVEGgCREGQCSEGUASEKUASAJUQaAJEQZAJIQZQBIQpQBIIlhRXndunUxffr0qK6ujoaGhti+ffsJHbdx48aoqKiIBQsWDOe0ADCmlR3lTZs2RXNzc7S0tMTOnTtjxowZMW/evHjjjTfe8bjXX389/uEf/iHmzp077MUCwFhWdpQfeOCBuPHGG2Pp0qXxoQ99KNavXx/nnHNOPProo8c9pr+/Pz7zmc/E3XffHRdccMG7WjAAjFVlRbmvry927NgRTU1Nv7uDceOiqakptm3bdtzjvvzlL8ekSZPihhtuOKHz9Pb2Rk9Pz5AbAIx1ZUX5wIED0d/fH3V1dUPG6+rqorOz85jHPPfcc/HII4/Ehg0bTvg8ra2tUVtbO3irr68vZ5kAcFoa1XdfHzp0KBYtWhQbNmyIiRMnnvBxK1eujO7u7sHbvn37RnGVAJDD+HImT5w4MSorK6Orq2vIeFdXV0yePPmo+T/96U/j9ddfj/nz5w+ODQwM/PbE48fHyy+/HBdeeOFRx5VKpSiVSuUsDQBOe2VdKVdVVcWsWbOivb19cGxgYCDa29ujsbHxqPkXX3xxvPDCC9HR0TF4+9SnPhVXX311dHR0eFoaAH5PWVfKERHNzc2xZMmSmD17dsyZMyfWrl0bhw8fjqVLl0ZExOLFi2PatGnR2toa1dXVcckllww5/rzzzouIOGocAM50ZUd54cKFsX///li1alV0dnbGzJkzo62tbfDNX3v37o1x4/xDYQBQroqiKIpTvYg/pqenJ2pra6O7uztqampO9XIAOMONVpdc0gJAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJDCvK69ati+nTp0d1dXU0NDTE9u3bjzt3w4YNMXfu3JgwYUJMmDAhmpqa3nE+AJypyo7ypk2borm5OVpaWmLnzp0xY8aMmDdvXrzxxhvHnL9169a47rrr4gc/+EFs27Yt6uvr45Of/GT8/Oc/f9eLB4CxpKIoiqKcAxoaGuLyyy+PBx98MCIiBgYGor6+Pm655ZZYsWLFHz2+v78/JkyYEA8++GAsXrz4hM7Z09MTtbW10d3dHTU1NeUsFwBG3Gh1qawr5b6+vtixY0c0NTX97g7GjYumpqbYtm3bCd3Hm2++GW+99Va8973vPe6c3t7e6OnpGXIDgLGurCgfOHAg+vv7o66ubsh4XV1ddHZ2ntB93HbbbTF16tQhYf9Dra2tUVtbO3irr68vZ5kAcFo6qe++XrNmTWzcuDGeeuqpqK6uPu68lStXRnd39+Bt3759J3GVAHBqjC9n8sSJE6OysjK6urqGjHd1dcXkyZPf8dj77rsv1qxZE9///vfj0ksvfce5pVIpSqVSOUsDgNNeWVfKVVVVMWvWrGhvbx8cGxgYiPb29mhsbDzucffee2/cc8890dbWFrNnzx7+agFgDCvrSjkiorm5OZYsWRKzZ8+OOXPmxNq1a+Pw4cOxdOnSiIhYvHhxTJs2LVpbWyMi4p/+6Z9i1apV8fjjj8f06dMHX3t+z3veE+95z3tG8KEAwOmt7CgvXLgw9u/fH6tWrYrOzs6YOXNmtLW1Db75a+/evTFu3O8uwL/xjW9EX19f/PVf//WQ+2lpaYkvfelL7271ADCGlP055VPB55QByCTF55QBgNEjygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkIQoA0ASogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkMawor1u3LqZPnx7V1dXR0NAQ27dvf8f5//Zv/xYXX3xxVFdXx0c+8pHYsmXLsBYLAGNZ2VHetGlTNDc3R0tLS+zcuTNmzJgR8+bNizfeeOOY859//vm47rrr4oYbbohdu3bFggULYsGCBfGTn/zkXS8eAMaSiqIoinIOaGhoiMsvvzwefPDBiIgYGBiI+vr6uOWWW2LFihVHzV+4cGEcPnw4vve97w2OfexjH4uZM2fG+vXrT+icPT09UVtbG93d3VFTU1POcgFgxI1Wl8aXM7mvry927NgRK1euHBwbN25cNDU1xbZt2455zLZt26K5uXnI2Lx58+Lpp58+7nl6e3ujt7d38Ofu7u6I+O0mAMCp9naPyryu/aPKivKBAweiv78/6urqhozX1dXFSy+9dMxjOjs7jzm/s7PzuOdpbW2Nu++++6jx+vr6cpYLAKPqf//3f6O2tnbE7q+sKJ8sK1euHHJ1ffDgwXjf+94Xe/fuHdEHf6bq6emJ+vr62Ldvn5cDRog9HVn2c+TZ05HV3d0d559/frz3ve8d0fstK8oTJ06MysrK6OrqGjLe1dUVkydPPuYxkydPLmt+RESpVIpSqXTUeG1trV+mEVRTU2M/R5g9HVn2c+TZ05E1btzIfrK4rHurqqqKWbNmRXt7++DYwMBAtLe3R2Nj4zGPaWxsHDI/IuLZZ5897nwAOFOV/fR1c3NzLFmyJGbPnh1z5syJtWvXxuHDh2Pp0qUREbF48eKYNm1atLa2RkTErbfeGldddVXcf//9ce2118bGjRvjxz/+cTz88MMj+0gA4DRXdpQXLlwY+/fvj1WrVkVnZ2fMnDkz2traBt/MtXfv3iGX81dccUU8/vjjceedd8btt98ef/EXfxFPP/10XHLJJSd8zlKpFC0tLcd8Spvy2c+RZ09Hlv0cefZ0ZI3Wfpb9OWUAYHT4t68BIAlRBoAkRBkAkhBlAEgiTZR9HeTIKmc/N2zYEHPnzo0JEybEhAkToqmp6Y/u/5mo3N/Rt23cuDEqKipiwYIFo7vA00y5+3nw4MFYtmxZTJkyJUqlUlx00UX+d/8Hyt3TtWvXxgc+8IE4++yzo76+PpYvXx6/+c1vTtJqc/vhD38Y8+fPj6lTp0ZFRcU7fl/D27Zu3Rof/ehHo1Qqxfvf//547LHHyj9xkcDGjRuLqqqq4tFHHy3++7//u7jxxhuL8847r+jq6jrm/B/96EdFZWVlce+99xYvvvhiceeddxZnnXVW8cILL5zkledU7n5ef/31xbp164pdu3YVu3fvLv72b/+2qK2tLf7nf/7nJK88r3L39G2vvfZaMW3atGLu3LnFX/3VX52cxZ4Gyt3P3t7eYvbs2cU111xTPPfcc8Vrr71WbN26tejo6DjJK8+r3D399re/XZRKpeLb3/528dprrxXPPPNMMWXKlGL58uUneeU5bdmypbjjjjuKJ598soiI4qmnnnrH+Xv27CnOOeecorm5uXjxxReLr3/960VlZWXR1tZW1nlTRHnOnDnFsmXLBn/u7+8vpk6dWrS2th5z/qc//eni2muvHTLW0NBQ/N3f/d2orvN0Ue5+/qEjR44U5557bvGtb31rtJZ42hnOnh45cqS44oorim9+85vFkiVLRPn3lLuf3/jGN4oLLrig6OvrO1lLPO2Uu6fLli0rPvGJTwwZa25uLq688spRXefp6ESi/MUvfrH48Ic/PGRs4cKFxbx588o61yl/+vrtr4NsamoaHDuRr4P8/fkRv/06yOPNP5MMZz//0JtvvhlvvfXWiP9D66er4e7pl7/85Zg0aVLccMMNJ2OZp43h7Od3v/vdaGxsjGXLlkVdXV1ccsklsXr16ujv7z9Zy05tOHt6xRVXxI4dOwaf4t6zZ09s2bIlrrnmmpOy5rFmpLp0yr8l6mR9HeSZYjj7+Yduu+22mDp16lG/YGeq4ezpc889F4888kh0dHSchBWeXoazn3v27In//M//jM985jOxZcuWePXVV+Pzn/98vPXWW9HS0nIylp3acPb0+uuvjwMHDsTHP/7xKIoijhw5EjfffHPcfvvtJ2PJY87xutTT0xO//vWv4+yzzz6h+znlV8rksmbNmti4cWM89dRTUV1dfaqXc1o6dOhQLFq0KDZs2BATJ0481csZEwYGBmLSpEnx8MMPx6xZs2LhwoVxxx13xPr160/10k5bW7dujdWrV8dDDz0UO3fujCeffDI2b94c99xzz6le2hntlF8pn6yvgzxTDGc/33bffffFmjVr4vvf/35ceumlo7nM00q5e/rTn/40Xn/99Zg/f/7g2MDAQEREjB8/Pl5++eW48MILR3fRiQ3nd3TKlClx1llnRWVl5eDYBz/4wejs7Iy+vr6oqqoa1TVnN5w9veuuu2LRokXx2c9+NiIiPvKRj8Thw4fjpptuijvuuGPEv5JwrDtel2pqak74KjkiwZWyr4McWcPZz4iIe++9N+65555oa2uL2bNnn4ylnjbK3dOLL744Xnjhhejo6Bi8fepTn4qrr746Ojo6or6+/mQuP53h/I5eeeWV8eqrrw7+cRMR8corr8SUKVPO+CBHDG9P33zzzaPC+/YfPYWvRCjbiHWpvPegjY6NGzcWpVKpeOyxx4oXX3yxuOmmm4rzzjuv6OzsLIqiKBYtWlSsWLFicP6PfvSjYvz48cV9991X7N69u2hpafGRqN9T7n6uWbOmqKqqKp544oniF7/4xeDt0KFDp+ohpFPunv4h774eqtz93Lt3b3HuuecWf//3f1+8/PLLxfe+971i0qRJxVe+8pVT9RDSKXdPW1painPPPbf413/912LPnj3Ff/zHfxQXXnhh8elPf/pUPYRUDh06VOzatavYtWtXERHFAw88UOzatav42c9+VhRFUaxYsaJYtGjR4Py3PxL1j//4j8Xu3buLdevWnb4fiSqKovj6179enH/++UVVVVUxZ86c4r/+678G/9tVV11VLFmyZMj873znO8VFF11UVFVVFR/+8IeLzZs3n+QV51bOfr7vfe8rIuKoW0tLy8lfeGLl/o7+PlE+Wrn7+fzzzxcNDQ1FqVQqLrjgguKrX/1qceTIkZO86tzK2dO33nqr+NKXvlRceOGFRXV1dVFfX198/vOfL/7v//7v5C88oR/84AfH/P/Ft/dwyZIlxVVXXXXUMTNnziyqqqqKCy64oPjnf/7nss/rqxsBIIlT/poyAPBbogwASYgyACQhygCQhCgDQBKiDABJiDIAJCHKAJCEKANAEqIMAEmIMgAkIcoAkMT/AwbAMwFP3iC4AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}