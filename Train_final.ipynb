{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bitang-Melyen-Tanulok/Csip_Csip/blob/main/Train_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQrSXadE4xMi",
        "outputId": "c291dbdc-e635-4372-e1cf-2d5600656a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Current folder: blrwar1\n",
            "Current folder: grenig1\n",
            "Current folder: junmyn1\n",
            "Current folder: pursun4\n",
            "Current folder: revbul\n",
            "Current folder: barswa\n",
            "Current folder: jerbus2\n",
            "Current folder: insowl1\n",
            "Current folder: lewduc1\n",
            "Current folder: plaflo1\n",
            "Current folder: putbab1\n",
            "Current folder: redspu1\n",
            "Current folder: rewlap1\n",
            "Current folder: rufbab3\n",
            "Current folder: sqtbul1\n",
            "Current folder: vefnut1\n",
            "Current folder: categr\n",
            "Current folder: forwag1\n",
            "Current folder: rerswa1\n",
            "Current folder: rutfly6\n",
            "\n",
            "X shape: (1769, 128, 313, 3)\n",
            "Y shape: (1769,)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/DeepLearning'\n",
        "spectrograms_path = os.path.join(path, 'ugyanaz?')  # Adott mappa elérési útvonala\n",
        "folders = os.listdir(spectrograms_path)\n",
        "\n",
        "for folder in folders:\n",
        "    print(f\"Current folder: {folder}\")\n",
        "    folder_path = os.path.join(spectrograms_path, folder)  # Mappa útvonal\n",
        "\n",
        "    # Fájlok bejárása a mappában\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    for file in files:\n",
        "        if file.endswith('.png'):  # Csak PNG fájlokat olvasunk be\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "\n",
        "            # Kép betöltése\n",
        "            img = image.load_img(file_path, color_mode='rgb')\n",
        "            img_array = image.img_to_array(img)  # Kép tömbbé alakítása\n",
        "            X.append(img_array)\n",
        "            Y.append(folder)\n",
        "\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "print(f\"\\nX shape: {X.shape}\")\n",
        "print(f\"Y shape: {Y.shape}\")\n",
        "\n",
        "#shuffle X and Y the same way\n",
        "permutation = np.random.permutation(len(X))\n",
        "X = X[permutation]\n",
        "Y = Y[permutation]\n",
        "\n",
        "test_split = 0.1\n",
        "valid_split = 0.1\n",
        "\n",
        "nb_samples = len(X)\n",
        "valid_start_idx = int(nb_samples*(1-valid_split-test_split))\n",
        "test_start_idx = int(nb_samples*(1-test_split))\n",
        "\n",
        "X_train = X[:valid_start_idx]\n",
        "Y_train = Y[:valid_start_idx]\n",
        "X_valid = X[valid_start_idx:test_start_idx]\n",
        "Y_valid = Y[valid_start_idx:test_start_idx]\n",
        "X_test = X[test_start_idx:]\n",
        "Y_test = Y[test_start_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#Since this is a multi-class classification task, I am converting labels to one-hot format:\n",
        "\n",
        "#First, the labels need to be converted into numerical values\n",
        "le = LabelEncoder()\n",
        "Y_train_encoded = le.fit_transform(Y_train)\n",
        "Y_valid_encoded = le.transform(Y_valid)\n",
        "Y_test_encoded = le.transform(Y_test)\n",
        "\n",
        "#Getting number of classes\n",
        "num_classes = len(le.classes_)\n",
        "print(f\"Class number= {num_classes}\")\n",
        "\n",
        "# Converting to one-hot encoding\n",
        "Y_train= to_categorical(Y_train_encoded, num_classes)\n",
        "Y_valid = to_categorical(Y_valid_encoded, num_classes)\n",
        "Y_test = to_categorical(Y_test_encoded, num_classes)"
      ],
      "metadata": {
        "id": "8Wr1YIvJXB2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbd6e38-2678-4753-d854-ea0bbd8b9a9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class number= 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "#Implementing early stopping, since there is no reason for it to learn further when val_loss isn't decreasing\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5, #If it doesn't improve for 10 epochs, it concludes\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "FUAtgZ4iuF9d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint # Importing the missing ModelCheckpoint class\n",
        "\n",
        "checkpoint_dir = '/content/drive/MyDrive/DeepLearning/checkpoints_sample'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_train_path = os.path.join(checkpoint_dir, 'model_train_{epoch:02d}_{val_loss:.4f}.keras')\n",
        "checkpoint_finetune_path = os.path.join(checkpoint_dir, 'model_finetune_{epoch:02d}_{val_loss:.4f}.keras')\n",
        "\n",
        "#Saving the latest model at each epoch, so that if colab collapses, we don't have to start from zero\n",
        "checkpoint_train = ModelCheckpoint(\n",
        "    checkpoint_train_path,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint_finetune = ModelCheckpoint(\n",
        "    checkpoint_finetune_path,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch',\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "2ykyRUDhuG5P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def return_latest_checkpoint_path(checkpoint_dir, fold_prefix):\n",
        "    model_files = os.listdir(checkpoint_dir)\n",
        "    # Filter files that match the fold_prefix\n",
        "    model_files = [f for f in model_files if f.startswith(fold_prefix)]\n",
        "    if not model_files:\n",
        "        print(\"No checkpoint found for fold.\")\n",
        "        return None, 0\n",
        "\n",
        "    # Extract epoch numbers using regex\n",
        "    epoch_numbers = []\n",
        "    for f in model_files:\n",
        "        match = re.search(r'_(\\d+)', f)  # Extract the number after fold prefix and before '_val_loss'\n",
        "        if match:\n",
        "            epoch_numbers.append((int(match.group(1)), f))\n",
        "\n",
        "    # Get the latest file by epoch number\n",
        "    latest_epoch, latest_file = max(epoch_numbers, key=lambda x: x[0])\n",
        "    latest_model_path = os.path.join(checkpoint_dir, latest_file)\n",
        "\n",
        "    return latest_model_path, latest_epoch"
      ],
      "metadata": {
        "id": "ZeMm1FEluNQv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet"
      ],
      "metadata": {
        "id": "IiGrBwwOuQiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a617f4ce-e425-42bb-e6b3-6bf498775b24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting efficientnet\n",
            "  Downloading efficientnet-1.1.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from efficientnet) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.12.1)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (11.0.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (2024.12.12)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (0.4)\n",
            "Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-applications, efficientnet\n",
            "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Implementing mixed precision training for faster outcomes\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Normalizing the data to fit EfficientNetV2B0\n",
        "X_train = preprocess_input(X_train)\n",
        "X_valid = preprocess_input(X_valid)\n",
        "\n",
        "# Setting the input shape based on the data\n",
        "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
        "\n",
        "# Loading the pre-trained EfficientNetV2B0 model\n",
        "base_model = EfficientNetV2B0(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=input_shape\n",
        ")\n",
        "\n",
        "# Defining a new model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Freezing the base model layers if we don't want to update them during training\n",
        "base_model.trainable = False\n",
        "\n",
        "# Adding the pre-trained model\n",
        "model.add(base_model)\n",
        "model.add(layers.GlobalAveragePooling2D())  # Selecting the best representations\n",
        "# Adding additional layers\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.4))  # Adding Dropout to prevent overfitting\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Loading saved model with the latest epoch froim 'model_train_{epoch:02d}_{val_loss:.4f}.keras'\n",
        "latest_checkpoint_path, initial_epoch = return_latest_checkpoint_path(checkpoint_dir, 'model_train')\n",
        "if latest_checkpoint_path:\n",
        "    model = load_model(latest_checkpoint_path)\n",
        "    print(\"Resumed training from the latest checkpoint.\")\n",
        "else:\n",
        "    initial_epoch = 0\n",
        "    print(\"Starting training from scratch.\")\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    epochs=20,\n",
        "    initial_epoch=initial_epoch,\n",
        "    batch_size=128, # try larger throughput 128, 256, original is 64\n",
        "    validation_data=(X_valid, Y_valid),\n",
        "    callbacks=[early_stopping, checkpoint_train],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fine-tuning the last 10 layers\n",
        "base_model.trainable = False\n",
        "for layer in base_model.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Loading saved model with the latest epoch froim 'model_train_{epoch:02d}_{val_loss:.4f}.keras'\n",
        "latest_checkpoint_path, initial_epoch = return_latest_checkpoint_path(checkpoint_dir, 'model_finetune')\n",
        "if latest_checkpoint_path:\n",
        "    model = load_model(latest_checkpoint_path)\n",
        "    print(\"Resumed fine-tuning from the latest checkpoint.\")\n",
        "else:\n",
        "    initial_epoch = 0\n",
        "    print(\"Starting fine-tuning from scratch.\")\n",
        "\n",
        "model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy']) # low learning rate\n",
        "\n",
        "fine_tuning_history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    epochs=5,\n",
        "    initial_epoch=initial_epoch,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_valid, Y_valid),\n",
        "    callbacks=[early_stopping, checkpoint_finetune],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Saving the results\n",
        "final_model_path = os.path.join(checkpoint_dir, f'final_model.keras')\n",
        "model.save(final_model_path)\n",
        "\n",
        "valid_loss, valid_accuracy = model.evaluate(X_valid, Y_valid)\n",
        "\n",
        "# Summary of the final model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "X5otFkcSuURW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "f16e561c-50ba-45b7-dc49-4ed625b470de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2c8fc94bf6da>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Loading saved model with the latest epoch froim 'model_train_{epoch:02d}_{val_loss:.4f}.keras'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mlatest_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_latest_checkpoint_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'model_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlatest_checkpoint_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nResults:\")\n",
        "print(f\"Validation Loss: {valid_loss}\")\n",
        "print(f\"Validation Accuracy: {valid_accuracy}\")"
      ],
      "metadata": {
        "id": "IG5spDQxY_e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Diplaying accuracy and loss on separate diagrams\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title(f'Fold {i + 1} Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title(f'Fold {i + 1} Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FdDH9g7BZCXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}